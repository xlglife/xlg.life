---
{"dg-publish":true,"dg-permalink":"growup100/062-无它，唯航海日志尔","permalink":"/growup100/062-无它，唯航海日志尔/","tags":["小洛哥成长笔记"],"noteIcon":"1","created":"2024-06-20","updated":"2024-06-20"}
---


> 大家好，我是小洛哥，一个刚刚开始每天写作的新人。
> 
> 日更 100 天，第 62 天。

![](http://img.xlg.life/images%2F2024%2F06%2F20%2F20240620232151-26ec5cedd6438147818b45d975976f44.png)

> GPT 告诉我：这句话出自《加勒比海盗：黑珍珠的诅咒》，原文是“无它，唯航海日志尔”。
> 
> 又在一本正经的胡说八道，哈！

## 01 生财思维，今日打卡

### 完成航线任务

- 阅读并学习异常值思维，在日志中记录思考过程

### 1、关于生财思维，我今天获得的新知识或新感悟是：

异常值就是和正常的信息，有明显区别的信息。**找到异常值，分析背后的原因，就能找到新的机会。** 看看它和平时的信息，究竟有哪些地方不一样？（时间、需求、平台、节日、热点等）

#### 榜样的力量

1. 生财圈友秦汉唐，刷抖音时发现儿童游乐的短视频，预计播放量超过 1000 万，深入分析后发现，但最重要的评论区里大量的加盟咨询，顿感发现了新大陆，经过分析这个模式爆火的原因后，他判断这个模式可以复制，做了两个月，招了 41 个代理商，变现 82 万，净收入 40 万；
2. 生财圈友代码君，在一次短视频航海群里，看到一个圈友发了个娱乐明星的视频，并表示有流量，难变现，于是代码君深入研究发现，有一些短视频并非直接变现，而是通过引流到公众号变现，于是马上着手像素级复制，结果发文当天就出了 1 单，后续放大后达到了月入 10 万
3. 生财圈友老黄牛，日常关注一些千粉小 V，哪条文章转发多，就学习哪条，必成大热，还分享了美团初期如何使用异常值的故事：美团有一个专门的监控系统，跟踪市场上所有的 APP，谁的日单量超过 1000，就拿过来抄，就这样饿了么进入了美美团的视野，王兴马上下达战略抄抄抄！就这样逐步美团最终逆袭了饿了么！

### 2、基于今天得到的知识点，我对生财思维的实际应用，有哪些思考：

这两天基本上没怎么看对标，也是因为前几天刚搜罗过一轮，但今天学习了异常值的角度后发现，之前看对标的方式还是有点粗，只是从粉丝数、点赞数等基本数据上去看的，然而并没有去分析评论内容，也确实平时没有什么看评论的习惯
今天的案例提醒到我了，可以从评论着手去发现新机会，不仅仅低粉爆款值得学习，更可以去从多个维度出发，看看能否找到有不一样的异常值

## 02 AI 编程（爬虫），今日打卡

### 完成航线任务

- 从 0 到 1 完整理解一个爬虫代码

### 1、我有好事要与大家分享

今天学习手册里「完整理解一份爬虫代码」部分，除了借助 AI 仔细阅读理解代码之外，还学习到了几个新的知识点：
1. `set()` 是集合数据类型，可以自动去重，即当存储数据进去时，如果已存在则不会重复添加，这个非常棒！之前搞 RPA 时，用列表、字典去重要来回转换好几次数据类型很麻烦，同时也学习到了 `set()` 的一些内置方法，如 `add`、`remove`、`discard`、`pop` 等等
2. `driver.find_elements(By.XPATH, '//img[@class="main_img img-hover"]')`，在抓取元素时，也可以使用 Xpath，好在 Xpath 这个技能在折腾 RPA 时已经比较熟练了

### 2、我今天想要复盘

 > 今天的任务相对简单，主要是完整理解一份爬虫代码，我将之前爬取百度图片的代码交给了 GPT，请他来帮我解释

1. **导入必要的库**：

	```python
	import os
	import time
	import requests
	from selenium import webdriver
	from selenium.webdriver.common.by import By
	from selenium.webdriver.common.keys import Keys
	from selenium.webdriver.chrome.service import Service
	from webdriver_manager.chrome import ChromeDriverManager
	```

   - `os`：这个模块提供了一种便携式的使用操作系统功能的方法。我们会用它来创建文件夹。
   - `time`：这个模块提供了与时间相关的各种功能，比如暂停执行代码（用在 `time.sleep`）。
   - `requests`：这是一个用于发送 HTTP 请求的库，方便我们下载图片。
   - `selenium`：这是一个用于 Web 自动化的工具，我们用它来控制浏览器。
   - `By` 和 `Keys`：这些是 Selenium 中的一些模块，用于定位网页元素和模拟键盘按键。
   - `Service` 和 `ChromeDriverManager`：用于启动和管理 Chrome 浏览器驱动。
2. **创建保存图片的文件夹**：

	```python
	if not os.path.exists('AIGC'):
	    os.makedirs('AIGC')
	```

   - `os.path.exists('AIGC')`：检查名为 'AIGC' 的文件夹是否存在。
   - `os.makedirs('AIGC')`：如果文件夹不存在，就创建这个文件夹。
3. **启动 Chrome 浏览器**：

	```python
	options = webdriver.ChromeOptions()
	options.add_argument('--start-maximized')
	driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
	```

   - `webdriver.ChromeOptions()`：创建一个 Chrome 浏览器的配置对象。
   - `options.add_argument('--start-maximized')`：设置浏览器启动时窗口最大化。
   - `webdriver.Chrome(...)`：使用指定的配置启动 Chrome 浏览器。`ChromeDriverManager().install()` 会自动下载和安装 Chrome 驱动。
4. **打开百度图片**：

	```python
	try:
	    driver.get('https://image.baidu.com/')
	```

   - `driver.get('https://image.baidu.com/')`：让浏览器打开百度图片的页面。
5. **输入关键字并点击搜索按钮**：

	```python
	    search_box = driver.find_element(By.NAME, 'word')
	    search_box.send_keys('AIGC')
	    search_box.send_keys(Keys.RETURN)
	```

   - `driver.find_element(By.NAME, 'word')`：通过元素的 `name` 属性找到搜索框。
   - `search_box.send_keys('AIGC')`：在搜索框中输入关键字 "AIGC"。
   - `search_box.send_keys(Keys.RETURN)`：模拟按下回车键，提交搜索。
6. **初始化计数器和 URL 集合**：

	```python
	    img_count = 0
	    img_urls = set()
	```

   - `img_count = 0`：初始化图片计数器为 0。
   - `img_urls = set()`：创建一个空的集合来存储图片的 URL。使用集合可以自动去重。
7. **开始爬取图片**：

	```python
	    while img_count < 100:
	        time.sleep(3)
	        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
	```

   - `while img_count < 100`：循环，直到爬取到 100 张图片为止。
   - `time.sleep(3)`：每次循环暂停 3 秒，等待页面加载更多内容。
   - `driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")`：执行 JavaScript 代码，将页面滚动到最底部，以加载更多图片。
8. **提取图片 URL**：

	```python
	        imgs = driver.find_elements(By.XPATH, '//img[@class="main_img img-hover"]')
	        for img in imgs:
	            if img_count >= 100:
	                break
	            src = img.get_attribute('src')
	            if src and src.startswith(('http', 'https')) and src not in img_urls:
	                img_urls.add(src)
	                img_count += 1
	```

   - `driver.find_elements(By.XPATH, '//img[@class="main_img img-hover"]')`：找到页面中所有符合条件的图片元素。
   - `for img in imgs`：遍历每个图片元素。
   - `if img_count >= 100`：如果已经爬取到 100 张图片，跳出循环。
   - `src = img.get_attribute('src')`：获取图片的 URL。
   - `if src and src.startswith(('http', 'https')) and src not in img_urls`：检查 URL 是否有效且未重复。
   - `img_urls.add(src)`：将图片 URL 添加到集合中。
   - `img_count += 1`：计数器加 1。
9. **下载图片**：

	```python
	    for idx, img_url in enumerate(img_urls):
	        try:
	            response = requests.get(img_url)
	            if response.status_code == 200:
	                with open(f'AIGC/{idx + 1}.jpg', 'wb') as f:
	                    f.write(response.content)
	                print(f'已下载第{idx + 1}张图片')
	        except Exception as e:
	            print(f'Failed to download image {img_url}: {e}')
	```

   - `for idx, img_url in enumerate(img_urls)`：遍历所有图片 URL。
   - `response = requests.get(img_url)`：发送 HTTP 请求下载图片。
   - `if response.status_code == 200`：检查请求是否成功。
   - `with open(f'AIGC/{idx + 1}.jpg', 'wb') as f`：以二进制写模式打开文件，文件名为图片序号。
   - `f.write(response.content)`：将图片内容写入文件。
   - `print(f'已下载第{idx + 1}张图片')`：打印下载成功的信息。
   - `except Exception as e`：捕捉下载过程中的异常，并打印错误信息。
10. **关闭浏览器**：

	```python
	finally:
	    driver.quit()
	```

    - `finally`：无论前面的代码是否有异常，这段代码都会执行。
    - `driver.quit()`：关闭浏览器并结束 WebDriver 会话。
